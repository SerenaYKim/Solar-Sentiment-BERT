{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.5"
    },
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuClass": "premium",
      "gpuType": "A100"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G8IYpSiMsiqb",
        "outputId": "49dd67f0-bc4c-4b86-90d6-dda5b8263e25"
      },
      "source": [
        "gpu_info = !nvidia-smi\n",
        "gpu_info = '\\n'.join(gpu_info)\n",
        "if gpu_info.find('failed') >= 0:\n",
        "  print('Select the Runtime > \"Change runtime type\" menu to enable a GPU accelerator, ')\n",
        "  print('and then re-execute this cell.')\n",
        "else:\n",
        "  print(gpu_info)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Thu Feb 22 00:01:53 2024       \n",
            "+---------------------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 535.104.05             Driver Version: 535.104.05   CUDA Version: 12.2     |\n",
            "|-----------------------------------------+----------------------+----------------------+\n",
            "| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                                         |                      |               MIG M. |\n",
            "|=========================================+======================+======================|\n",
            "|   0  NVIDIA A100-SXM4-40GB          Off | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   30C    P0              47W / 350W |      2MiB / 40960MiB |      0%      Default |\n",
            "|                                         |                      |             Disabled |\n",
            "+-----------------------------------------+----------------------+----------------------+\n",
            "                                                                                         \n",
            "+---------------------------------------------------------------------------------------+\n",
            "| Processes:                                                                            |\n",
            "|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n",
            "|        ID   ID                                                             Usage      |\n",
            "|=======================================================================================|\n",
            "|  No running processes found                                                           |\n",
            "+---------------------------------------------------------------------------------------+\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5Vj-3pXFsifl",
        "outputId": "4265762f-7299-48d3-a520-5a00b4124082"
      },
      "source": [
        "from psutil import virtual_memory\n",
        "ram_gb = virtual_memory().total / 1e9\n",
        "print('Your runtime has {:.1f} gigabytes of available RAM\\n'.format(ram_gb))\n",
        "\n",
        "if ram_gb < 20:\n",
        "  print('To enable a high-RAM runtime, select the Runtime > \"Change runtime type\"')\n",
        "  print('menu, and then select High-RAM in the Runtime shape dropdown. Then, ')\n",
        "  print('re-execute this cell.')\n",
        "else:\n",
        "  print('You are using a high-RAM runtime!')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Your runtime has 89.6 gigabytes of available RAM\n",
            "\n",
            "You are using a high-RAM runtime!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fC5AG-pesiVy",
        "outputId": "e1e571d4-99e1-464d-f53f-9995573ef6ba"
      },
      "source": [
        "import tensorflow as tf\n",
        "import torch\n",
        "\n",
        "# If there's a GPU available...\n",
        "if torch.cuda.is_available():\n",
        "\n",
        "    # Tell PyTorch to use the GPU.\n",
        "    device = torch.device(\"cuda\")\n",
        "\n",
        "    print('There are %d GPU(s) available.' % torch.cuda.device_count())\n",
        "\n",
        "    print('We will use the GPU:', torch.cuda.get_device_name(0))\n",
        "\n",
        "# If not...\n",
        "else:\n",
        "    print('No GPU available, using the CPU instead.')\n",
        "    device = torch.device(\"cpu\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "There are 1 GPU(s) available.\n",
            "We will use the GPU: NVIDIA A100-SXM4-40GB\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "Collapsed": "false",
        "id": "R2hhY37ZsYoM"
      },
      "source": [
        "import torch\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from tqdm.notebook import tqdm\n",
        "from functools import reduce\n",
        "from datetime import datetime\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import f1_score, confusion_matrix"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 01. Imports"
      ],
      "metadata": {
        "id": "hpjzuDfbX3Fs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers"
      ],
      "metadata": {
        "id": "Abjp2AJcX2lg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import TensorDataset\n",
        "from transformers import RobertaTokenizer, RobertaConfig, RobertaForSequenceClassification, AdamW"
      ],
      "metadata": {
        "id": "grKy20uHX5bJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print('Loading RoBERTa tokenizer...')\n",
        "tokenizer = RobertaTokenizer.from_pretrained('roberta-base', do_lower_case=True)"
      ],
      "metadata": {
        "id": "HkQXjMudX5XN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = RobertaForSequenceClassification.from_pretrained(\"roberta-base\",\n",
        "                                                      num_labels=len(label_dict),\n",
        "                                                      output_attentions=False,\n",
        "                                                      output_hidden_states=False)"
      ],
      "metadata": {
        "id": "zE73bRH9ZBjR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.load_state_dict(torch.load('/content/finetuned_RoBERTa_epoch_10.model', map_location=torch.device('cpu')))"
      ],
      "metadata": {
        "id": "nNVPX5ctYVQ8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 02. Implementation (Takes 6+ hours on A100)"
      ],
      "metadata": {
        "id": "S0zfBcrINEET"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "years = [\"2015\", \"2016\", \"2017\", \"2018\", \"2019\", \"2020\", \"2021\", \"2022\"]\n",
        "\n",
        "for year in years:\n",
        "  df_full = pd.read_csv(\"../\" + year + \".csv\", lineterminator='\\n')\n",
        "  df_pred = df_full\n",
        "  df_pred[\"data_type\"] = \"pred\"\n",
        "  df_pred[\"label\"] = 1\n",
        "\n",
        "  # Using the tokenizer to encode the pred data\n",
        "  encoded_df_pred = tokenizer.batch_encode_plus(\n",
        "      df_pred[df_pred.data_type=='pred'].text.values,\n",
        "      add_special_tokens=True,\n",
        "      return_attention_mask=True,\n",
        "      pad_to_max_length=True,\n",
        "      truncation=True,\n",
        "      max_length=256,\n",
        "      return_tensors='pt'\n",
        "  )\n",
        "\n",
        "  # Separating the input_ids, attention_masks and labels from the encoded data\n",
        "\n",
        "  input_ids_pred = encoded_df_pred['input_ids']\n",
        "  attention_masks_pred = encoded_df_pred['attention_mask']\n",
        "  labels_pred = torch.tensor(df_pred[df_pred.data_type=='pred'].label.values)\n",
        "\n",
        "  # Creating a Tensor Dataset from the input_ids, attention_masks and labels\n",
        "  dataset_pred = TensorDataset(input_ids_pred, attention_masks_pred, labels_pred)\n",
        "\n",
        "  # Creating a Dataloader from the Tensor Dataset for pred data\n",
        "  dataloader_pred = DataLoader(dataset_pred,\n",
        "                              sampler=SequentialSampler(dataset_pred),\n",
        "                              batch_size=batch_size)\n",
        "\n",
        "  _, predictions, true_vals = evaluate(dataloader_pred)\n",
        "\n",
        "  def predicted_list_generate(preds, labels):\n",
        "    label_dict_inverse = {v: k for k, v in label_dict.items()}\n",
        "\n",
        "    preds_flat = np.argmax(preds, axis=1).flatten()\n",
        "    labels_flat = labels.flatten()\n",
        "    df_label_created = pd.DataFrame()\n",
        "\n",
        "    predicted = preds_flat.tolist()\n",
        "\n",
        "    return predicted\n",
        "\n",
        "  prediction = predicted_list_generate(predictions, true_vals)\n",
        "  df_pred['predicted_label'] = prediction\n",
        "  df_pred = df_pred.drop([\"label\", \"data_type\"], axis=1)\n",
        "\n",
        "  df_pred['category_pred'] = np.nan\n",
        "\n",
        "  #Creates a new dictionary with keys and values reversed from an existing dictionary 'label_dict'\n",
        "  label_dict_reversed = {v: k for k, v in label_dict.items()}\n",
        "\n",
        "  # Fills in the 'category_pred' column with values from the 'label_dict_reversed' dictionary based on the corresponding values in the 'predicted_label' column.\n",
        "  df_pred['category_pred']= df_pred[\"predicted_label\"].map(label_dict_reversed).fillna(df_pred[\"category_pred\"])\n",
        "\n",
        "  df_pred = df_pred.drop([\"predicted_label\"], axis=1)\n",
        "\n",
        "  # Save\n",
        "  df_pred.to_csv(\"../\" + year + \"_Predicted.csv\", index=False)\n"
      ],
      "metadata": {
        "id": "fzFZgTsYBp2w"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}